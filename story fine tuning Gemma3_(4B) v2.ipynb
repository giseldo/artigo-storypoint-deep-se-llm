{"cells":[{"cell_type":"markdown","metadata":{"id":"4UKC3-0y6LMQ"},"source":["To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n","<div class=\"align-center\">\n","<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n","<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n","<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n","</div>\n","\n","To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n","\n","You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"]},{"cell_type":"markdown","metadata":{"id":"jSNeusdt6LMT"},"source":["### News"]},{"cell_type":"markdown","metadata":{"id":"EsBRDCT36LMT"},"source":["Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n","\n","Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"]},{"cell_type":"code","source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n","    !pip install --no-deps unsloth"],"metadata":{"id":"u-aEIoif-CKt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rPWlfCak6LMT"},"source":["### Installation"]},{"cell_type":"code","source":["from unsloth import FastModel\n","import torch\n","import pandas as pd\n","from datasets import Dataset\n","\n","import re\n","\n","def remove_html_tags(text):\n","    if isinstance(text, str):\n","        # Remove HTML tags\n","        clean_text = re.sub(r'<[^>]+>', '', text)\n","        # Remove {html} prefix if present\n","        clean_text = re.sub(r'^{html}', '', clean_text)\n","        return clean_text.strip()\n","    return text\n","\n","def remove_urls(text):\n","    if isinstance(text, str):\n","        # Remove URLs\n","        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n","        clean_text = re.sub(url_pattern, '', text)\n","        return clean_text.strip()\n","    return text\n","\n","fourbit_models = [\n","    # 4bit dynamic quants for superior accuracy and low memory use\n","    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n","    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n","    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n","    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n","\n","    # Other popular models!\n","    \"unsloth/Llama-3.1-8B\",\n","    \"unsloth/Llama-3.2-3B\",\n","    \"unsloth/Llama-3.3-70B\",\n","    \"unsloth/mistral-7b-instruct-v0.3\",\n","    \"unsloth/Phi-4\",\n","] # More models at https://huggingface.co/unsloth\n","\n","model, tokenizer = FastModel.from_pretrained(\n","    model_name = \"unsloth/gemma-3-4b-it\",\n","    max_seq_length = 2048, # Choose any for long context!\n","    load_in_4bit = True,  # 4 bit quantization to reduce memory\n","    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n","    full_finetuning = False, # [NEW!] We have full finetuning now!\n","    # token = \"hf_...\", # use one if using gated models\n",")\n","\n","model = FastModel.get_peft_model(\n","    model,\n","    finetune_vision_layers     = False, # Turn off for just text!\n","    finetune_language_layers   = True,  # Should leave on!\n","    finetune_attention_modules = True,  # Attention good for GRPO\n","    finetune_mlp_modules       = True,  # SHould leave on always!\n","\n","    r = 8,           # Larger = higher accuracy, but might overfit\n","    lora_alpha = 8,  # Recommended alpha == r at least\n","    lora_dropout = 0,\n","    bias = \"none\",\n","    random_state = 3407,\n",")\n","\n","from unsloth.chat_templates import get_chat_template\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"gemma-3\",\n",")\n","\n","# Load the local CSV dataset\n","df = pd.read_csv(\"dataset/deep-se.csv\")\n","\n","df = df.dropna()\n","\n","# Take a random sample of the data\n","sample_size = 100\n","df = df.sample(n=min(sample_size, len(df)), random_state=42)\n","\n","# Remove HTML tags from title and description columns\n","df['title'] = df['title'].apply(remove_html_tags)\n","df['description'] = df['description'].apply(remove_html_tags)\n","# Remove URLs from title and description columns\n","df['title'] = df['title'].apply(remove_urls)\n","df['description'] = df['description'].apply(remove_urls)\n","\n","df['context'] = df['title'] + \" \" + df['description']\n","\n","# Convert the DataFrame to a format suitable for training\n","def prepare_conversation(row):\n","    return {\n","        \"conversations\": [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [{\"type\": \"text\", \"text\": row[\"context\"]}]\n","            },\n","            {\n","                \"role\": \"assistant\",\n","                \"content\": [{\"type\": \"text\", \"text\": row[\"storypoint\"]}]\n","            }\n","        ]\n","    }\n","\n","# Convert DataFrame to list of conversations\n","conversations = df.apply(prepare_conversation, axis=1).tolist()\n","\n","# Create a HuggingFace dataset\n","dataset = Dataset.from_list(conversations)\n","\n","from unsloth.chat_templates import standardize_data_formats\n","dataset = standardize_data_formats(dataset)\n","\n","def apply_chat_template(examples):\n","    texts = tokenizer.apply_chat_template(examples[\"conversations\"])\n","    return { \"text\" : texts }\n","pass\n","dataset = dataset.map(apply_chat_template, batched = True)\n","\n","dataset[100][\"text\"]\n","\n","from trl import SFTTrainer, SFTConfig\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    eval_dataset = None, # Can set up evaluation!\n","    args = SFTConfig(\n","        dataset_text_field = \"text\",\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n","        warmup_steps = 5,\n","        # num_train_epochs = 1, # Set this for 1 full training run.\n","        max_steps = 30,\n","        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        report_to = \"none\", # Use this for WandB etc\n","    ),\n",")\n","\n","from unsloth.chat_templates import train_on_responses_only\n","trainer = train_on_responses_only(\n","    trainer,\n","    instruction_part = \"<start_of_turn>user\\n\",\n","    response_part = \"<start_of_turn>model\\n\",\n",")\n","\n","tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])\n","\n","tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")\n","\n","trainer_stats = trainer.train()\n","\n","from unsloth.chat_templates import get_chat_template\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"gemma-3\",\n",")\n","messages = [{\n","    \"role\": \"user\",\n","    \"content\": [{\n","        \"type\" : \"text\",\n","        \"text\" : \"Continue the sequence: 1, 1, 2, 3, 5, 8,\",\n","    }]\n","}]\n","text = tokenizer.apply_chat_template(\n","    messages,\n","    add_generation_prompt = True, # Must add for generation\n",")\n","outputs = model.generate(\n","    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n","    max_new_tokens = 64, # Increase for longer outputs!\n","    # Recommended Gemma-3 settings!\n","    temperature = 1.0, top_p = 0.95, top_k = 64,\n",")\n","tokenizer.batch_decode(outputs)\n","\n","\n","messages = [{\n","    \"role\": \"user\",\n","    \"content\": [{\"type\" : \"text\", \"text\" : \"Why is the sky blue?\",}]\n","}]\n","text = tokenizer.apply_chat_template(\n","    messages,\n","    add_generation_prompt = True, # Must add for generation\n",")\n","\n","from transformers import TextStreamer\n","_ = model.generate(\n","    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n","    max_new_tokens = 64, # Increase for longer outputs!\n","    # Recommended Gemma-3 settings!\n","    temperature = 1.0, top_p = 0.95, top_k = 64,\n","    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",")\n","\n","\n","model.save_pretrained(\"gemma-3\")  # Local saving\n","tokenizer.save_pretrained(\"gemma-3\")\n","# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n","# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n","\n","\n","if False:\n","    from unsloth import FastModel\n","    model, tokenizer = FastModel.from_pretrained(\n","        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n","        max_seq_length = 2048,\n","        load_in_4bit = True,\n","    )\n","\n","messages = [{\n","    \"role\": \"user\",\n","    \"content\": [{\"type\" : \"text\", \"text\" : \"What is Gemma-3?\",}]\n","}]\n","text = tokenizer.apply_chat_template(\n","    messages,\n","    add_generation_prompt = True, # Must add for generation\n",")\n","\n","from transformers import TextStreamer\n","_ = model.generate(\n","    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n","    max_new_tokens = 64, # Increase for longer outputs!\n","    # Recommended Gemma-3 settings!\n","    temperature = 1.0, top_p = 0.95, top_k = 64,\n","    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",")\n","\n","if False: # Change to True to save finetune!\n","    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)\n","\n","if False: # Change to True to upload finetune\n","    model.push_to_hub_merged(\n","        \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n","        token = \"hf_...\"\n","    )\n","\n","if False: # Change to True to save to GGUF\n","    model.save_pretrained_gguf(\n","        \"gemma-3-finetune\",\n","        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n","    )\n","\n","if False: # Change to True to upload GGUF\n","    model.push_to_hub_gguf(\n","        \"gemma-3-finetune\",\n","        quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n","        repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n","        token = \"hf_...\",\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"Tjc2aN4s98yl","executionInfo":{"status":"error","timestamp":1747782725513,"user_tz":240,"elapsed":214,"user":{"displayName":"Giseldo Neo","userId":"08640009273611156104"}},"outputId":"3c114bef-3206-455b-b3a2-36800cc2aee4"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'unsloth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1f21e26106f2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":[],"metadata":{"id":"ry-L9yOQ99Gg"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb","timestamp":1747782693085}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}